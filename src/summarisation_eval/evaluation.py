"""evaluation.py
=================================================
A **minimal, fullyâ€‘annotated** helper for evaluating an abstractiveâ€‘summarisation
model (BARTâ€‘Largeâ€‘CNN by default) on a JSONâ€‘Lines dataset.  The script exposes a
single public API â€“ `evaluate_file` â€“ which:

1. Loads every row from a *JSONL* file that stores
   * `body`   â€“ the full article text to summarise, and
   * `summary` â€“ the humanâ€‘written reference abstract.
2. Generates a model summary for the *entire* article while respecting the
   1â€¯024â€‘token context of BARTâ€‘large via a slidingâ€‘window, mapâ€‘reduce strategy.
3. Computes corpusâ€‘level ROUGEâ€‘1/2/L and BERTScore metrics.
4. Returns a pair `(pred_df, metrics)` so that downstream notebooks can inspect
   perâ€‘row predictions **and** aggregate scores.

Designed to be light on dependencies, boilerplate, and hidden state â€“ drop it
into any project and call

```python
from mini_summarisation_eval_commented import evaluate_file
pred_df, metrics = evaluate_file("evaluation.jsonl", device=0)
```

All functions are documented with **docstrings** and annotated with **inline
comments** for quick comprehension.
"""

from __future__ import annotations  # â†© allow forward type references (Py<3.11)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Standard library â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from pathlib import Path
from statistics import mean
from typing import List, Tuple

import evaluate  # ğŸ¤— Evaluate â€“ ROUGE & BERTScore

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Thirdâ€‘party scientific stack â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import pandas as pd  # DataFrame I/O and manipulation
import torch  # Detect CUDA / place model on GPU
from transformers import (
    BartForConditionalGeneration,  # Model checkpoint loader
    BartTokenizer,  # Tokeniser for BART
    pipeline,  # Highâ€‘level HF task wrapper
)

# =============================================================================
# 1.  Model helper â€“ build a *summarization* pipeline
# =============================================================================

def build_summariser(
    model_name: str = "facebook/bart-large-cnn",
    device: int | str | None = None,
    *,  # force keywordâ€‘only for the tuning knobs below
    repetition_penalty: float = 1.15,
    no_repeat_ngram_size: int = 3,
    length_penalty: float = 1.1,
):
    """Create a HuggingFace **summarization** pipeline.

    Parameters
    ----------
    model_name
        Seq2Seq checkpoint with â‰¤1â€¯024â€‘token context; defaults to BARTâ€‘Largeâ€‘CNN.
    device
        `0`  â†’ first CUDA GPU Â· `-1` â†’ CPU Â· `None` (default) â†’ autodetect.
    repetition_penalty, no_repeat_ngram_size, length_penalty
        Decodingâ€‘time knobs that reduce verbatim repetition and favour slightly
        longer abstracts.

    Returns
    -------
    tuple[pipeline, BartTokenizer]
        * The readyâ€‘toâ€‘call HF pipeline.
        * The matching tokenizer (needed for tokenâ€‘window chunking).
    """

    # 1.  Tokeniser â€“ enforces the 1â€¯024 token window of BARTâ€‘large
    tokenizer = BartTokenizer.from_pretrained(model_name)
    tokenizer.model_max_length = 1024

    # 2.  Model weights â€“ downloaded / cached by transformers
    model = BartForConditionalGeneration.from_pretrained(model_name)

    # 3.  Device autotune â€“ pick GPU if available and caller didnâ€™t override
    if device is None:
        device = 0 if torch.cuda.is_available() else -1

    # 4.  Wrap everything in a taskâ€‘specific pipeline for convenience
    summariser = pipeline(
        "summarization",
        model=model,
        tokenizer=tokenizer,
        device=device,
        repetition_penalty=repetition_penalty,
        no_repeat_ngram_size=no_repeat_ngram_size,
        length_penalty=length_penalty,
    )

    return summariser, tokenizer

# =============================================================================
# 2.  Tokenâ€‘safe summarisation â€“ mapâ€‘reduce over 1â€¯k+ token articles
# =============================================================================

def _chunk(
    article: str,
    tok: BartTokenizer,
    *,
    window: int = 1024,
    overlap: int = 200,
):
    # 1ï¸âƒ£  Get raw content token IDs only
    ids = tok(article, truncation=False, add_special_tokens=False)["input_ids"]
    step = window - overlap

    for i, start in enumerate(range(0, len(ids), step)):
        # 2ï¸âƒ£  Slice content IDs
        slice_ids = ids[start : start + window]

        # 3ï¸âƒ£  Decode back to text
        txt = tok.decode(slice_ids, skip_special_tokens=True)

        # 4ï¸âƒ£  Re-encode content only
        re_ids = tok(txt, truncation=False, add_special_tokens=False)["input_ids"]

        # 5ï¸âƒ£  Print lengthsâ€”these *should* match exactly
        print(f"[DEBUG] window {i}:")
        print(f"  content IDs slice      â†’ {len(slice_ids)} tokens")
        print(f"  re-tokenized content   â†’ {len(re_ids)} tokens")
        if len(re_ids) != len(slice_ids):
            print("  âš ï¸  MISMATCH in content token counts!")

        yield txt


def summarise(
    article: str,
    summariser,       # HF pipeline
    tok: BartTokenizer,
    *,
    window: int = 1024,
    overlap: int = 200,
    chunk_len: int = 60,
    final_len: int = 120,
) -> str:
    """"Map-reduce summarisation with verbose tokenâ€count debug info."""

  # â”€â”€â”€ EARLY BAILOUT FOR SHORT INPUTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    raw_ids = tok(article, truncation=False)["input_ids"]
    print(f"Article is {len(raw_ids)} tokens long; window={window}")
    if len(raw_ids) <= window:
        # Single pass at the 'final' length: no map / no reduce.
        print("â†’ fits in one window; doing one final summarisation and returning\n")
        return summariser(
            article,
            max_length=final_len,
            min_length=max(40, final_len // 2),
            do_sample=False,
            truncation=True,
        )[0]["summary_text"].strip()

    # â”€â”€â”€ OTHERWISE: TRUE MAP-REDUCE FLOW â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1. Create sliding-window chunks
    chunks = list(_chunk(article, tok, window=window, overlap=overlap))
    print(f"â†’ {len(chunks)} chunks generated")

    # 2. MAP: summarise each chunk
    blurbs = []
    for i, chunk in enumerate(chunks):
        ids = tok(chunk, truncation=False)["input_ids"]
        print(f"[MAP] chunk {i}: {len(ids)} tokens â†’ generating summary")
        try:
            out = summariser(
                chunk,
                max_length=chunk_len,
                min_length=15,
                do_sample=False,
                truncation=True,
            )[0]["summary_text"].strip()
        except Exception as e:
            print(f"âš ï¸  Exception in MAP chunk {i}: {e}")
            raise
        blurbs.append(out)

    # 3. If for some reason only one chunk remains (shouldnâ€™t happen here),
    #    you could either return it directly or fall through to REDUCE.
    if len(blurbs) == 1:
        return blurbs[0]

    # 4. REDUCE: fuse the blurbs
    bullet_prompt = (
        f"Summarise the following bullet-point section summaries into ONE coherent "
        f"paragraph (â‰¤{final_len} tokens):\n\n" +
        "\n".join(f"â€¢ {b}" for b in blurbs)
    )
    bp_ids = tok(bullet_prompt, truncation=False)["input_ids"]
    print(f"[REDUCE] bullet_prompt is {len(bp_ids)} tokens")

    try:
        final = summariser(
            bullet_prompt,
            max_length=final_len,
            min_length=max(40, final_len // 2),
            do_sample=False,
            truncation=True,
        )[0]["summary_text"].strip()
    except Exception as e:
        print(f"âš ï¸  Exception in REDUCE prompt: {e}")
        raise

    return final

# =============================================================================
# 3.  Metrics â€“ ROUGEâ€‘1/2/L and (baselineâ€‘rescaled) BERTScore
# =============================================================================

def compute_metrics(
    preds: List[str],
    refs: List[str],
    *,
    use_baseline: bool = True,
) -> dict:
    """Return corpusâ€‘level ROUGE and BERTScore averages.

    Parameters
    ----------
    preds / refs
        Must be equal length â€“ iâ€‘th hypothesis compared against iâ€‘th reference.
    use_baseline
        If *True*, BERTScore is baselineâ€‘rescaled (scores can be negative).

    Returns
    -------
    dict with two keys:
        * ``rouge`` â€“ whatever ğŸ¤— *evaluate*'s ROUGE loader returns
        * ``bertscore`` â€“ dict with mean precision/recall/F1 floats
    """

    # Compute ROUGEâ€‘1/2/L (precision, recall, F1) â€“ microâ€‘averaged by *evaluate*
    rouge = evaluate.load("rouge").compute(predictions=preds, references=refs)

    # Compute BERTScore and average over the dataset manually (macroâ€‘average)
    bs = evaluate.load("bertscore").compute(
        predictions=preds,
        references=refs,
        lang="en",
        rescale_with_baseline=use_baseline,
    )

    return {
        "rouge": rouge,
        "bertscore": {
            "precision": mean(bs["precision"]),
            "recall": mean(bs["recall"]),
            "f1": mean(bs["f1"]),
        },
    }

# =============================================================================
# 4.  Topâ€‘level public API â€“ one function to rule them all
# =============================================================================

def evaluate_file(
    path: str | Path,
    *,
    device: int | str | None = None,
    use_baseline: bool = True,
) -> Tuple[pd.DataFrame, dict]:
    """Run the full evaluation loop on *path* and return `(pred_df, metrics)`.

    Parameters
    ----------
    path
        Path to a JSONâ€‘Lines file (*one JSON object per line*) with `body` and
        `summary` keys.
    device
        `0/1/â€¦` â†’ that CUDA GPU Â· `-1` â†’ CPU Â· `None` â†’ autodetect.
    use_baseline
        Whether to baselineâ€‘rescale BERTScore.

    Returns
    -------
    pred_df : pandas.DataFrame
        Original rows plus a new `prediction` column holding model summaries.
    metrics : dict
        Output of :func:`compute_metrics` â€“ corpus ROUGE & BERTScore.
    """

    # 1.  Load dataset (assumes JSONâ€‘Lines / one article per line)
    df = pd.read_json(path, lines=True)

    # 2.  Build model once â€“ reused for every row # WE NEED TO CHANGE IT HERE SO WE CAN PASS WHICH MODEL TO RUN
    summariser, tok = build_summariser(device=device)

    # 3.  Generate summaries â€“ pandas apply keeps things tidy
    df = df.copy()  # avoid SettingWithCopyWarning
    df["prediction"] = df["body"].apply(lambda txt: summarise(txt, summariser, tok))

    # 4.  Aggregate metrics
    metrics = compute_metrics(
        preds=df["prediction"].tolist(),
        refs=df["summary"].tolist(),
        use_baseline=use_baseline,
    )

    return df, metrics

# =============================================================================
# 5.  CLI demo â€“ only executed when you `python mini_summarisation_evalâ€¦`.
#     Keeps the file usable as both a module *and* a tiny script.
# =============================================================================
if __name__ == "__main__":
    # â”€â”€â”€ Quick & dirty smoke test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    pred_df, metrics = evaluate_file(
        path="data/eval.json", 
        device=0,                 # set to â€“1 for CPU
        use_baseline=True,
    )